# 图片相似度计算方法总结

## 传统方法
### 余弦相似度
&emsp;&emsp;把图片表示成一个向量，通过计算向量之间的余弦距离来表征两张图片的相似度

```python
# -*- coding: utf-8 -*-
# !/usr/bin/env python
# 余弦相似度计算
from PIL import Image
from numpy import average, dot, linalg
# 对图片进行统一化处理
def get_thum(image, size=(64, 64), greyscale=False):
    # 利用image对图像大小重新设置, Image.ANTIALIAS为高质量的
    image = image.resize(size, Image.ANTIALIAS)
    if greyscale:
        # 将图片转换为L模式，其为灰度图，其每个像素用8个bit表示
        image = image.convert('L')
    return image
# 计算图片的余弦距离
def image_similarity_vectors_via_numpy(image1, image2):
    image1 = get_thum(image1)
    image2 = get_thum(image2)
    images = [image1, image2]
    vectors = []
    norms = []
    for image in images:
        vector = []
        for pixel_tuple in image.getdata():
            vector.append(average(pixel_tuple))
        vectors.append(vector)
        # linalg=linear（线性）+algebra（代数），norm则表示范数
        # 求图片的范数
        norms.append(linalg.norm(vector, 2))
    a, b = vectors
    a_norm, b_norm = norms
    # dot返回的是点积，对二维数组（矩阵）进行计算
    res = dot(a / a_norm, b / b_norm)
    return res
image1 = Image.open('010.jpg')
image2 = Image.open('011.jpg')
cosin = image_similarity_vectors_via_numpy(image1, image2)
print('图片余弦相似度', cosin)
```


### 哈希算法
&emsp;&emsp;哈希算法实现图片相似度计算,实现图片相似度比较的哈希算法有三种：均值哈希算法，差值哈希算法，感知哈希算法.
1. 均值哈希算法
&emsp;&emsp;一张图片就是一个二维信号，它包含了不同频率的成分。亮度变化小的区域是低频成分，它描述大范围的信息。而亮度变化剧烈的区域（比如物体的边缘）就是高频的成分，它描述具体的细节。或者说高频可以提供图片详细的信息，而低频可以提供一个框架。 而一张大的，详细的图片有很高的频率，而小图片缺乏图像细节，所以都是低频的。所以我们平时的下采样，也就是缩小图片的过程，实际上是损失高频信息的过程。均值哈希算法就是利用图片的低频信息。
具体步骤：
（1）缩小尺寸：将图片缩小到8x8的尺寸，总共64个像素。这一步的作用是去除图片的细节，只保留结构、明暗等基本信息，摒弃不同尺寸、比例带来的图片差异。
（2）简化色彩：将缩小后的图片，转为64级灰度。也就是说，所有像素点总共只有64种颜色。
（3）计算平均值：计算所有64个像素的灰度平均值
（4）比较像素的灰度：将每个像素的灰度，与平均值进行比较。大于或等于平均值，记为1；小于平均值，记为0。
（5）计算哈希值：将上一步的比较结果，组合在一起，就构成了一个64位的整数，这就是这张图片的指纹。组合的次序并不重要，只要保证所有图片都采用同样次序就行了。
&emsp;&emsp;最后得到两张图片的指纹信息后，计算两组64位数据的汉明距离，即对比数据不同的位数，不同位数越少，表明图片的相似度越大。
分析： 均值哈希算法计算速度快，不受图片尺寸大小的影响，但是缺点就是对均值敏感，例如对图像进行伽马校正或直方图均衡就会影响均值，从而影响最终的hash值。

2. 感知哈希算法
&emsp;&emsp;感知哈希算法是一个比均值哈希算法更为健壮的一种算法，与均值哈希算法的区别在于感知哈希算法是通过DCT（离散余弦变换）来获取图片的低频信息。
离散余弦变换（DCT）是种图像压缩算法，它将图像从像素域变换到频率域。然后一般图像都存在很多冗余和相关性的，所以转换到频率域之后，只有很少的一部分频率分量的系数才不为0，大部分系数都为0（或者说接近于0）。经过DCT变换后的系数矩阵从左上角到右下角频率越来越高，因此图片的能量主要保留在左上角的低频系数上了。
具体步骤：
（1）缩小尺寸：pHash以小图片开始，但图片大于8x8，32x32是最好的。这样做的目的是简化了DCT的计算，而不是减小频率。
（2）简化色彩：将图片转化成灰度图像，进一步简化计算量。
（3）计算DCT：计算图片的DCT变换，得到32x32的DCT系数矩阵。
（4）缩小DCT：虽然DCT的结果是32x32大小的矩阵，但我们只要保留左上角的8x8的矩阵，这部分呈现了图片中的最低频率。
（5）计算平均值：如同均值哈希一样，计算DCT的均值。
（6）计算hash值：这是最主要的一步，根据8x8的DCT矩阵，设置0或1的64位的hash值，大于等于DCT均值的设为”1”，小于DCT均值的设为“0”。组合在一起，就构成了一个64位的整数，这就是这张图片的指纹。
&emsp;&emsp;分析： 结果并不能告诉我们真实性的低频率，只能粗略地告诉我们相对于平均值频率的相对比例。只要图片的整体结构保持不变，hash结果值就不变。能够避免伽马校正或颜色直方图被调整带来的影响。对于变形程度在25%以内的图片也能精准识别。

3. 差值哈希算法
&emsp;&emsp;比pHash，dHash的速度要快的多，相比aHash，dHash在效率几乎相同的情况下的效果要更好，它是基于渐变实现的。
主要步骤：
（1）缩小尺寸：收缩到8x9（高x宽）的大小，一遍它有72的像素点
（2）转化为灰度图：把缩放后的图片转化为256阶的灰度图。
（3）计算差异值：dHash算法工作在相邻像素之间，这样每行9个像素之间产生了8个不同的差异，一共8行，则产生了64个差异值
（4）获得指纹：如果左边的像素比右边的更亮，则记录为1，否则为0.

代码实现如下：
```python
import cv2
import numpy as np
#感知哈希算法
def pHash(image): 
    image = cv2.resize(image,(32,32), interpolation=cv2.INTER_CUBIC) 
    image = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY) 
#     cv2.imshow('image', image)
#     cv2.waitKey(0)
#     cv2.destroyAllWindows()
    # 将灰度图转为浮点型，再进行dct变换 
    dct = cv2.dct(np.float32(image))
#     print(dct)
    # 取左上角的8*8，这些代表图片的最低频率 
    # 这个操作等价于c++中利用opencv实现的掩码操作 
    # 在python中进行掩码操作，可以直接这样取出图像矩阵的某一部分 
    dct_roi = dct[0:8,0:8]  
    avreage = np.mean(dct_roi) 
    hash = [] 
    for i in range(dct_roi.shape[0]): 
        for j in range(dct_roi.shape[1]): 
            if dct_roi[i,j] > avreage: 
                hash.append(1) 
            else: 
                hash.append(0) 
    return hash

#均值哈希算法
def aHash(image):
    #缩放为8*8
    image=cv2.resize(image,(8,8),interpolation=cv2.INTER_CUBIC)
    #转换为灰度图
    image=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
    avreage = np.mean(image) 
    hash = [] 
    for i in range(image.shape[0]): 
        for j in range(image.shape[1]): 
            if image[i,j] > avreage: 
                hash.append(1) 
            else: 
                hash.append(0) 
    return hash

#差值感知算法
def dHash(image):
    #缩放9*8
    image=cv2.resize(image,(9,8),interpolation=cv2.INTER_CUBIC)
    #转换灰度图
    image=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)
#     print(image.shape)
    hash=[]
    #每行前一个像素大于后一个像素为1，相反为0，生成哈希
    for i in range(8):
        for j in range(8):
            if image[i,j]>image[i,j+1]:
                hash.append(1)
            else:
                hash.append(0)
    return hash

#计算汉明距离
def Hamming_distance(hash1,hash2): 
    num = 0
    for index in range(len(hash1)): 
        if hash1[index] != hash2[index]: 
            num += 1
    return num
if __name__ == "__main__":
    image_file1 = './data/cartoon1.jpg'
    image_file2 = './data/cartoon3.jpg'
    img1 = cv2.imread(image_file1)
    img2 = cv2.imread(image_file2)
    hash1 = pHash(img1)
    hash2 = pHash(img2)
    dist = Hamming_distance(hash1, hash2)
    #将距离转化为相似度
    similarity = 1 - dist * 1.0 / 64 
    print(dist)
    print(similarity)

```

### 直方图
1.方法描述：

按照某种距离度量的标准对两幅图像的直方图进行相似度的测量。

2.图像直方图丰富的图像细节信息，反映了图像像素点的概率分布情况，统计每一个像素点强度值具有的像素个数。

3.优点：计算量比较小。

4.缺点： 直方图反应的是图像灰度值得概率分布，并没有图像的空间位置信息在里面，因此，会出现误判；比如纹理结构相同，但明暗不同的图像，应该相似度很高，但实际结果是相似度很低，而纹理结构不同，但明暗相近的图像，相似度却很高。

计算步骤：

1.将图片resize，得到相同大小的图片

2.将图片灰度，灰度后图片的像素在[0-255]之间

3.计算图片的直方图数据，统计相同像素点的概率分布

4.根据相关性计算公式，计算两个图片直方图的相关性。


```python
import cv2
def calculate(image1, image2):
    # 灰度直方图算法
    # 计算单通道的直方图的相似值
    hist1 = cv2.calcHist([image1], [0], None, [256], [0.0, 255.0])
    hist2 = cv2.calcHist([image2], [0], None, [256], [0.0, 255.0])
    # 计算直方图的重合度
    degree = 0
    for i in range(len(hist1)):
        if hist1[i] != hist2[i]:
            degree = degree + (1 - abs(hist1[i] - hist2[i]) / max(hist1[i], hist2[i]))
        else:
            degree = degree + 1
    degree = degree / len(hist1)
    return degree
 
def classify_hist_with_split(image1, image2, size=(256, 256)):
    # RGB每个通道的直方图相似度
    # 将图像resize后，分离为RGB三个通道，再计算每个通道的相似值
    image1 = cv2.resize(image1, size)
    image2 = cv2.resize(image2, size)
    sub_image1 = cv2.split(image1)
    sub_image2 = cv2.split(image2)
    sub_data = 0
    for im1, im2 in zip(sub_image1, sub_image2):
        sub_data += calculate(im1, im2)
    sub_data = sub_data / 3
    return sub_data
```


### SSIM（结构相似度度量)

SSIM算法原理

SSIM(structural similarity)，结构相似性，是一种衡量两幅图像相似度的指标。SSIM算法主要用于检测两张相同尺寸的图像的相似度、或者检测图像的失真程度。原论文中，SSIM算法主要通过分别比较两个图像的亮度，对比度，结构，然后对这三个要素加权并用乘积表示。

[ref](https://blog.csdn.net/hedgehog__/article/details/107257755)

是一种全参考的图像质量评价指标，分别从亮度、对比度、结构三个方面度量图像相似性。SSIM取值范围[0, 1]，值越大，表示图像失真越小。

代码:
```python
import cv2
from skimage.metrics import structural_similarity

img1 = cv2.imread('1.png')
img2 = cv2.imread('2.png')
img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0])
ssim_score = structural_similarity(img1, img2, data_range=255, multichannel=True)
```


### 互信息（Mutual Information）

互信息(Mutual Information)是信息论里一种有用的信息度量，它可以看成是一个随机变量中包含的关于另一个随机变量的信息量，或者说是一个随机变量由于已知另一个随机变量而减少的不肯定性
归一化互信息(NMI)，就是将互信息放在[0,1]之间

代码:
```
from sklearn import metrics as mr

img1 = cv2.imread('1.png')
img2 = cv2.imread('2.png')
img2 = cv2.resize(img2, (img1.shape[1], img1.shape[0])
nmi = mr.normalized_mutual_info_score(img1.reshape(-1), img2.reshape(-1))
```

### pixelmatch

利用像素之间的匹配来计算相似度

```
#第一步:
pip install pixelmatch
#第二步:
from PIL import Image
from pixelmatch.contrib.PIL import pixelmatch
img_a = Image.open("a.png")
img_b = Image.open("b.png")
img_diff = Image.new("RGBA", img_a.size)
# note how there is no need to specify dimensions
mismatch = pixelmatch(img_a, img_b, img_diff, includeAA=True)
img_diff.save("diff.png")
```


## 深度学习方法

思路:深度特征提取+特征向量相似度计算

参考:
2.1 论文1

名称: Learning to Compare Image Patches via Convolutional Neural Networks
- 论文：https://arxiv.org/pdf/1504.03641.pdf
- 讲解：https://blog.csdn.net/hjimce/article/details/50098483
- code: https://github.com/szagoruyko/cvpr15deepcompare

2.2 论文2

名称: The Unreasonable Effectiveness of Deep Features as a Perceptual Metric
- 论文：https://arxiv.org/pdf/1801.03924.pdf
- 讲解: https://blog.csdn.net/weixin_41605888/article/details/88887416
- code: https://github.com/richzhang/PerceptualSimilarity






















